<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://dev-owner.github.io/blog/</id>
    <title>dev-owner's blog Blog</title>
    <updated>2022-11-20T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://dev-owner.github.io/blog/"/>
    <subtitle>dev-owner's blog Blog</subtitle>
    <icon>https://dev-owner.github.io/blog/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[\[AWS EKS\] EKS on Kafka vs MSK]]></title>
        <id>aws-eks-on-kafka-vs-msk</id>
        <link href="https://dev-owner.github.io/blog/aws-eks-on-kafka-vs-msk"/>
        <updated>2022-11-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[aws eks on kafka vs msk]]></summary>
        <content type="html"><![CDATA[<p>Kafka는 이벤트 기반 아키텍처나 MSA 등등 현대 아키텍처 메세지 브로커로 빼놓을 수 없는 요소입니다.</p><p>EKS 또한 대부분의 애플리케이션을 컨테이너로 배포하는 와중에 거의 필수적으로 사용하는 쿠버네티스를 보다 더 편리하게 사용할 때 많이 사용하는 요소입니다.</p><p>그러면 Kafka를 EKS에 올려서 사용하는 케이스에 대해 어떤 방법이 있는지, 장단점이 무엇인지 알아봅시다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="어떤-방법들이-있을까">어떤 방법들이 있을까?<a class="hash-link" href="#어떤-방법들이-있을까" title="제목으로 바로 가기">​</a></h2><p>EKS(K8S)에 카프카를 배포하는 방식에는 여러가지가 있을 것 같습니다.</p><ol><li><a href="https://github.com/strimzi/strimzi-kafka-operator" target="_blank" rel="noopener noreferrer">Manage Kafka on Kubernetes: Strimzi</a></li><li><a href="https://github.com/banzaicloud/koperator" target="_blank" rel="noopener noreferrer">Banzai Cloud</a></li><li><a href="https://docs.confluent.io/operator/current/co-deploy-cfk.html" target="_blank" rel="noopener noreferrer">Confluent</a></li><li><a href="https://artifacthub.io/packages/helm/bitnami/kafka" target="_blank" rel="noopener noreferrer">Bitnami Kafka Helm chart</a></li></ol><p>위 방식들 말고도 여러 방법들이 있을 수 있습니다 :)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="eks-on-kafka에서-생각해-볼-수-있는-점">EKS on Kafka에서 생각해 볼 수 있는 점<a class="hash-link" href="#eks-on-kafka에서-생각해-볼-수-있는-점" title="제목으로 바로 가기">​</a></h2><p>아무래도 직접 클러스터를 관리하다 보니 그 과정에서 배우는건 많을 것 같습니다. 그리고 모든것이 Admin의 통제 하에 관리되니 자유도나 디버깅 환경 또한 관리형 서비스보다는 편할 듯 싶습니다.</p><p>그러나 그 이외 부분에서는 제 지식의 한계인지 더 좋은점은 생각하지 못했습니다. 대부분의 케이스에서 heavy lifting한 작업을 대신해주는 관리형 서비스가 낫다는 생각을 떨칠수가 없네요..</p><p>구축하는 레퍼런스는 아래와 같은 사이트들이 있습니다.</p><ul><li><a href="https://medium.com/@JinnaBalu/kafka-cluster-on-amezon-eks-cluster-5850d67ae723" target="_blank" rel="noopener noreferrer">https://medium.com/@JinnaBalu/kafka-cluster-on-amezon-eks-cluster-5850d67ae723</a></li><li><a href="https://portworx.com/run-ha-kafka-amazon-elastic-container-service-kubernetes/" target="_blank" rel="noopener noreferrer">https://portworx.com/run-ha-kafka-amazon-elastic-container-service-kubernetes/</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="amazon-managed-streaming-for-apache-kafkamsk">Amazon Managed Streaming for Apache Kafka(MSK)<a class="hash-link" href="#amazon-managed-streaming-for-apache-kafkamsk" title="제목으로 바로 가기">​</a></h2><p>AWS에서는 Kafka를 managed service로 제공하는 MSK가 있습니다.</p><p>위에서 잠깐 얘기했지만, 사실 대부분의 상황에서 MSK를 쓰는게 나을 것 같다는 생각입니다. 직접 설치 운영은 업데이트나 관리 측면에서 어려움을 겪을 확률이 크기 때문이고, Heavy lifting을 피할 수 있으면 피하는게 낫지 않을까라는 생각입니다.</p><p>그리고 관리 측면을 제외하더라도, EKS와 달리 MSK는 <strong>inter-az 데이터 전송 비용이 무료</strong>이기 때문에 이러한 부분또한 고려해보면 좋습니다.</p><h1>결론</h1><p>아무래도 요즘 대부분의 케이스에서 직접 클러스터를 구축해서 사용하는 것 보다는 관리형 서비스를 사용하는게 여러가지 측면에서 효율성이 좋은 것 같습니다. 단, 회사의 규모가 너무 커서 On-premise에 직접 클러스터를 구축해서 관리해야 하는 상황이나 회사의 상황에 맞춰 특정 기능을 커스터마이징 해야 하는 상황 같은 경우 직접 해야 할 수도 있다고 생각합니다.</p>]]></content>
        <author>
            <name>Jaewoo Sung</name>
            <uri>https://github.com/dev-owner</uri>
        </author>
        <category label="AWS" term="AWS"/>
        <category label="EKS" term="EKS"/>
        <category label="K8S" term="K8S"/>
        <category label="kubernetes" term="kubernetes"/>
        <category label="docker" term="docker"/>
        <category label="kafka" term="kafka"/>
        <category label="msk" term="msk"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[실리콘밸리에서 날아온 데이터 엔지니어링 컨퍼런스]]></title>
        <id>review-programmers-data-engineering-conference</id>
        <link href="https://dev-owner.github.io/blog/review-programmers-data-engineering-conference"/>
        <updated>2022-11-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[프로그래머스 실리콘밸리 데이터 엔지니어링 코스에서 주최한 컨퍼런스 후기]]></summary>
        <content type="html"><![CDATA[<h3 class="anchor anchorWithStickyNavbar_LWe7" id="배경">배경<a class="hash-link" href="#배경" title="제목으로 바로 가기">​</a></h3><p>제가 처음 <a href="https://school.programmers.co.kr/learn/courses/14982" target="_blank" rel="noopener noreferrer">실리콘밸리에서 날아온 데이터 엔지니어링 코스</a>를 알게 된 건 아래 Youtube EO Channel에 올라온 한기용님 영상을 시청한 이후 입니다. 총 3부까지 있는 이 영상을 보고 나서 여러가지로 많은 생각을 하게 되었습니다. 혹시 아직 안보셨다면 한번쯤 가볍게 시청해보시는 것도 추천 드립니다.</p><p><img loading="lazy" src="/blog/assets/images/2022-11-10-1-c9a1a06bca769dd736fa5a910997c01e.png" width="2250" height="1314" class="img_ev3q"></p><p>기존에 4년정도 데이터 엔지니어링 업무를 대기업에서 했었는데, 다른 곳에서는 어떻게 하고 있는지 궁금하기도 하고 내가 기존에 몰랐던 것들을 혹시 배울 수 있지 않을까 하여 코스를 신청하게 되었습니다. 듣고 보니 역시 기대를 저버리지 않고 강의가 나름 알차게 코스가 구성되어 있어 많은 걸 배울 수 있었던 것 같습니다. 역시 세상은 넓고 배울것은 많습니다 :)</p><p>이 코스는 2022년 10월 기준 벌써 10기를 운영할 만큼 관련 종사자분들이 많이 듣고 계시기도 한데 이번에 코스 수강생들 대상으로 기용님이 데이터 엔지니어링 <a href="https://school.programmers.co.kr/learn/courses/15230" target="_blank" rel="noopener noreferrer">컨퍼런스를</a> 열어주셔서 다녀온 후기를 작성해보려 합니다.</p><p><img loading="lazy" src="/blog/assets/images/2022-11-10-2-20c06d0595f680eaabc7a0eca90878c7.png" width="1310" height="1250" class="img_ev3q">
(출처 : <a href="https://www.linkedin.com/in/hannahhejang/" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/in/hannahhejang/</a>)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="목차">목차<a class="hash-link" href="#목차" title="제목으로 바로 가기">​</a></h3><p>금일 컨퍼런스 목차입니다.</p><ol><li>Airflow 환경 고도화하기 (<a href="https://www.linkedin.com/in/ACoAAB0dV2QBZEoHPDMHVUaWR3F-okOKbAsuqhQ" target="_blank" rel="noopener noreferrer">hoyeon lee</a>)</li><li>공공데이터 적재하기 (<a href="https://www.linkedin.com/in/ACoAADdQHFoBvXqcwhKQG5YBRI0MNzRqGv_9j7w" target="_blank" rel="noopener noreferrer">Eunji Yi</a>)</li><li>스타트업에서 데이터기반 의사결정 구조 만들기 (<a href="https://www.linkedin.com/in/ACoAAAJgCuMBC6ob4tTNYE290unr86fVRLD7blM" target="_blank" rel="noopener noreferrer">Minjong Kim</a>)</li><li>참석자 네트워킹</li></ol><p>비록 시간대가 퇴근 후 저녁시간대였지만 1번부터 3번까지 흥미로운 주제로 가득했고 이는 컨퍼런스에 참가할 충분한 동기부여가 되었습니다. 참석자간 네트워킹 시간 같은 경우, 제가 한번도 경험이 없기 때문에 사실 한국 특성상 서먹서먹하고 어색한 분위기가 흐르다가 끝나지 않을까 막연한 예상을 하고 일단 들어갔습니다.</p><p><img loading="lazy" src="/blog/assets/images/2022-11-10-3-fe3ad1315de62c2c29f7ed0135f18c71.png" width="1202" height="1096" class="img_ev3q">
(출처 : <a href="https://www.linkedin.com/in/hannahhejang/" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/in/hannahhejang/</a>)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="airflow-환경-고도화하기">Airflow 환경 고도화하기<a class="hash-link" href="#airflow-환경-고도화하기" title="제목으로 바로 가기">​</a></h3><p>금일 컨퍼런스 첫번째 세션입니다. 쏘카의 <a href="https://www.linkedin.com/in/ACoAAB0dV2QBZEoHPDMHVUaWR3F-okOKbAsuqhQ" target="_blank" rel="noopener noreferrer">Grab</a>님이 발표를 진행해 주셨습니다.</p><p>GKE에서 K8S에 Airflow를 운영중이셨고, 통합 저장소로 BigQuery를 사용중이셨습니다. Airflow는 외부 저장소에서 BigQuery로 데이터를 적재할때 Batch 작업을 하기 위해 주로 사용했네요.</p><p>Airflow 개발환경에 대한 이야기를 많이 말씀 해주셨습니다. 초기 아키텍처는 Git과 연계하여 DAG 버전관리와 동시에 Airflow에 배포되는 구조를 가져간 것 같구요. 운영환경 및 다른 동료들과 독립적인 환경을 구축하기 위해 Git Branch를 따로 가져가고 push되면 buddyworks, argocd를 통해 전용 airflow가 생성되어 테스트 및 검증 후 운영 merge되는 구조였다고 합니다.</p><p>이런 구조에서 발생했던 자원 낭비, 불편한 환경, 긴 피드백 루프등의 많은 이슈로 인해 개발 환경 파이프라인을 docker compose 기반 로컬 노트북 환경으로 개선을 했다고 합니다. GCP Service Account를 통합 인증 수단으로 활용하였고, DAG Parsing Optimization을 위해 .airflowignore를 적극적으로 활용하셨다고 해요.</p><p>이러한 개선 구조를 통해 클러스터를 띄우는 시간을 많이 단축시키고 피드백 루프를 단축시켜 생산성을 많이 향상시킬 수 있었다고 합니다.</p><p>이어서 테스트 환경에 대한 이야기가 나왔는데, DAG 기본 문법 등을 체크하기 위해 pytest를 활용하셨고, pre commit hook을 사용하셨다고 합니다. Github Action에서 컨벤션, 테스트, 검증 등을 추가로 진행했다고 하네요.</p><p>꿀팁들도 여러가지 소개해 주셨는데, 그 중에 CleanUp DAG를 만들어서 주기적으로 TI나 DagRun등의 정보를 삭제하여 전반적인 cluster 반응성 등을 향상시킨 점이 인상깊었습니다.</p><p>모니터링, DAG 운영 알람, 보안 및 RBAC 적용, 외부 접속 정보에 대한 secret 저장소 활용, gcp secret manager 적용 등 많은 내용을 얘기해 주셨지만 시간이 많지는 않은 관계로 깊이는 다루지 못했던것 같습니다.</p><p>이후 위 내용들을 포함한 전반적인 내용을 <a href="https://tech.socarcorp.kr/data/2022/11/09/advanced-airflow-for-databiz.html" target="_blank" rel="noopener noreferrer">SOCAR Tech Blog</a>에서 소개해 주셔서 관심있으시면 한번 보시는걸 추천 드립니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="공공데이터-적재하기">공공데이터 적재하기<a class="hash-link" href="#공공데이터-적재하기" title="제목으로 바로 가기">​</a></h3><p>공공데이터 적재관련해서는 뱅크샐러드의 <a href="https://www.linkedin.com/in/ACoAADdQHFoBvXqcwhKQG5YBRI0MNzRqGv_9j7w" target="_blank" rel="noopener noreferrer">Eunji Yi</a>님이 발표해주셨는데요, 처음 소개에서 NASA에서 일한 경력이 있으신게 아주 인상깊었습니다.</p><p>이후에 뱅크샐러드에서 공공데이터 수집 관련해서 고군분투기를 말씀해 주셨는데, 본격적으로 공공데이터를 사용하기 위해 고려해야 할 것들이 얼마나 많은지 알 수 있었습니다.</p><p>저는 이전에 한번 업무에서 공공데이터 수집을 해봤는데도 불구하고, 제대로 여러기관의 데이터를 사용하기 위해 필요한것들을 이번에 처음 알았는데요, 대표적으로 기억나는 어려운 점들은 아래와 같았습니다.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1. 정의가 같은 컬럼이라도, 발행하는 기관이나 API에 따라 이름이 다를 수 있다.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. Enum으로 관리되어야 할 것 같은 상수값들이 역시 기관이나 API에 따라 이름이 다를 수 있다.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. API명세가 아닌 2번과 같은 값 같은 경우는 내용이 언제 바뀔지 모른다.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. 값이 기관에서만 알아볼 수 있는 특정 코드로 되어있는 경우가 대다수이다.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5. 4번과 같은 코드는 해당 API를 제공하는 곳이 아니라 저 멀리 이상한 다른 기관 사이트의 깊숙히 숨겨진 어딘가에 있다.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6. 공공기관에 문의는 한세월이다.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="클립보드에 코드 복사" title="복사" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>뱅크샐러드에서는 위의 1번~6번을 해결하기 위해 중간 Layer를 두어 잘 추상화하여 사용하고 계신다고 하였습니다.</p><p>사실 이런 부분이 실제 데이터를 본격적으로 사용하고자 할 때 크나큰 어려움을 겪는 부분이죠.
이런 데이터 표준과 같은 내용은 이미 제공하는 측에서 관리하여 이용자가 편하게 이용할 수 있게끔 관리해야 한다고 생각합니다. (많은 기관에서 데이터를 open하고 있는것은 아주 긍정적이지만, 아직 갈길이 먼 것 같습니다)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="스타트업에서-데이터기반-의사결정-구조-만들기">스타트업에서 데이터기반 의사결정 구조 만들기<a class="hash-link" href="#스타트업에서-데이터기반-의사결정-구조-만들기" title="제목으로 바로 가기">​</a></h3><p>마지막 세션은 NOUL의 <a href="https://www.linkedin.com/in/ACoAAAJgCuMBC6ob4tTNYE290unr86fVRLD7blM" target="_blank" rel="noopener noreferrer">Minjong Kim</a>님이 발표해주셨습니다.</p><p>스타트업에서 어떤 문제를 발견하고, 해당 문제를 그냥 지나치지 않고 데이터를 기반으로 의사결정하여 개선해나가는 과정을 말씀해 주셨습니다.
일을 하다 보니 데이터가 동료들이 작업하고 있는 환경의 데이터가 여기저기 파편화 되어 있고, 버전이 맞지 않으며 이로인해 비즈니스 임팩트까지 발생하는 것을 파악하셨고, 주 업무 분야가 아님에도 불구하고 이런것을 개선하기 위해 아래와 같은 과정을 진행하셨다고 합니다.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">1. NAS, Google Spreadsheet 기반 파일 사용 추적 후 DB에 정규화하여 데이터 사용 및 정합성 측면 개선</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">2. ML 모델 평가 추론 환경 개선</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">3. S3 + Sagemaker + DB</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">4. Inference를 반복적으로 수행하지 않게 개선</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">5. 통일되지 않은 데이터 타입, 변수명 등 전수조사 및 통일</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">6. 이러한 작업을 통한 데이터 가시성 확보</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="클립보드에 코드 복사" title="복사" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>사실 데이터쪽 업무를 하시는 분들이라면 아시겠지만, 당연히 필요한 일을 여러가지 이유로 하지 못하는 경우가 많습니다. 위의 사례를 통해 문제점 발견부터 의사결정권자와 동료들을 설득하는 과정, 개선하는 작업, 이후 시각화를 통한 추가 개선 사항 도출까지 많은 부분을 배운 것 같습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="네트워킹">네트워킹<a class="hash-link" href="#네트워킹" title="제목으로 바로 가기">​</a></h3><p>네트워킹 같은 경우 사람이 50명 가량 되어 여러 팀으로 나누어 진행하였습니다. 팀마다 발표하신 연사분들이 10분씩 돌아가며 질답을 받아주시고, 중간중간 자유롭게 주변 사람들하고 네트워킹 하는 시간을 가졌는데, 서먹할 것이라는 예상과 다르게 다들 엄청 적극적으로 네트워킹을 하셨던 것 같습니다. (순간 여기가 한국이 맞나 생각이 들었습니다 ^^;) 덕분에 소심한 저도 조금씩 질문도 하고 명함도 주고받고 했던 것 같네요.</p><p>아래는 제가 주로 질문했던 질문입니다.</p><ol><li><p>(Grab님에게) Socar에서 airflow dev 환경 구성할 때 Dev용 DW는 어떻게 구성하는지?</p><ul><li>(답변) Dev용 DW가 따로 있고 Test를 하는 사람이 데이터를 그때마다 임의로 구성하여 테스트</li><li>제가 궁금했던건 데이터가 다른 데이터를 복잡하게 의존하는 경우가 많아서 그런 경우 기존 DW와 비슷하게 구성해야 하는 경우가 있고 결국 효율성을 위해 운영환경과 비슷하게 구성해야 하는 경우가 많은데, 어떻게 하고있는지 궁금해서 여쭤봤습니다. 역시 운영환경과 비슷하게 하는건 리소스의 문제가 있기 때문에 작업자가 약간 힘들더라도 개별로 데이터를 구성하여 테스트를 한다고 하네요.</li></ul></li><li><p>(Grab님에게) BigQuery의 장점?</p><ul><li>(답변) 대충 넣어도 사용하기 편하고, 성능이 좋다. 비용은 모르겠다.</li><li>대충 넣어도 사용하기 편한건 제가 직접 경험해보지 않아 모르겠지만 성능은 AWS Redshift Serverless와 별 차이 없는게 벤치마크로 확인되어 인식의 차이가 아닌가 싶습니다. 비용도 AWS 대비 BigQuery가 비싼 케이스가 많은 것 같은데 사용 편의성에 묻혀서 많은 고객들이 그냥 사용하는 것 같기도 하네요.</li></ul></li><li><p>(이은지님에게) 공공데이터용 중간 Layer를 오픈소스로 공개하실 생각은 없는지?</p><ul><li>(답변) 회사 정책에 따라 확인해보겠다.</li><li>회사 자산이니 당연히 안될거라 생각하긴 했습니다..ㅎㅎ</li></ul></li></ol><p>여러 스타트업, 중견기업, 대기업에 다니시는분들이 골고루 계셨던 것 같고 취업준비생 분들도 간혹 계셨던 것 같습니다. 평소에 자주 쓰던 앱을 개발한 회사에서 오신분들을 보니 신기하기도 하고 짧지만 궁금했던 것들을 물어보며 그들의 생각도 조금이나마 들어볼 수 있던 좋은 시간이었습니다.</p><p>네트워킹을 1시간이나 잡아서 너무 많은 시간을 잡은 것 아닌가라는 생각을 했었는데, 마지막에는 다들 시간이 모자라서 허겁지겁 명함만 교환하고 아쉽게 헤어졌던 것 같네요.</p>]]></content>
        <author>
            <name>Jaewoo Sung</name>
            <uri>https://github.com/dev-owner</uri>
        </author>
        <category label="data" term="data"/>
        <category label="data engineering" term="data engineering"/>
        <category label="airflow" term="airflow"/>
        <category label="공공 데이터" term="공공 데이터"/>
    </entry>
</feed>